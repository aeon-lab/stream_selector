{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "azlulfFDhvVp",
   "metadata": {
    "id": "azlulfFDhvVp"
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a59f78-2a4f-4ad8-9be3-16c035585a92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73a59f78-2a4f-4ad8-9be3-16c035585a92",
    "outputId": "c233315a-9a93-4ed8-9701-41bb70835c95",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip install pyxdf && echo \"Installation Successful (pyxdf)\" || echo \"Installation Failed (pyxdf)\"\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install --upgrade pip\n",
    "# !{sys.executable} -m pip list --outdated --format=freeze | grep -v '^\\-e' | cut -d = -f 1 | xargs -n1 {sys.executable} -m pip install -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f95c76d-d8d0-4841-afac-fe79b73270f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In a Jupyter cell - using ! to run shell commands\n",
    "# !pip install --upgrade numpy pandas matplotlib scikit-learn seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b44fcb-dd7d-4b56-a99c-033e7f4f6a3c",
   "metadata": {
    "id": "a8b44fcb-dd7d-4b56-a99c-033e7f4f6a3c"
   },
   "outputs": [],
   "source": [
    "import pyxdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import shutil\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YYt0GZq8TDN-",
   "metadata": {
    "id": "YYt0GZq8TDN-"
   },
   "source": [
    "# F(x) XDF Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4fbb85-3f46-47e3-ba34-113cf2c0484d",
   "metadata": {
    "id": "ee4fbb85-3f46-47e3-ba34-113cf2c0484d"
   },
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# Channel Label Extraction\n",
    "# ===========================================================\n",
    "\n",
    "def extract_channel_labels(stream):\n",
    "    try:\n",
    "        # Navigate the nested metadata structure to find channel entries\n",
    "        channel_entries = stream['info']['desc'][0]['channels'][0]['channel']\n",
    "\n",
    "        # Handle both single channel (dict) and multiple channels (list of dicts)\n",
    "        if not isinstance(channel_entries, list):\n",
    "            channel_entries = [channel_entries]\n",
    "\n",
    "        labels, types, units = [], [], []\n",
    "\n",
    "        # Extract information from each channel entry\n",
    "        for i, ch in enumerate(channel_entries):\n",
    "            # Get label (try 'label' first, fallback to 'name', finally generic)\n",
    "            label = ch.get('label', ch.get('name', [f\"Channel {i}\"]))[0]\n",
    "            signal_type = ch.get('type', [''])[0]\n",
    "            unit = ch.get('unit', [''])[0]\n",
    "\n",
    "            labels.append(label)\n",
    "            types.append(signal_type)\n",
    "            units.append(unit)\n",
    "\n",
    "        return labels, types, units\n",
    "\n",
    "    except (KeyError, IndexError, TypeError):\n",
    "        # Fallback when metadata is missing or malformed\n",
    "        # Use channel_count from stream info to generate generic labels\n",
    "        num_channels = stream['info'].get('channel_count', ['0'])[0]\n",
    "        try:\n",
    "            num_channels = int(num_channels)\n",
    "        except ValueError:\n",
    "            num_channels = 1\n",
    "\n",
    "        # Generate generic channel labels\n",
    "        labels = [f\"Channel {i}\" for i in range(num_channels)]\n",
    "        return labels, [''] * num_channels, [''] * num_channels\n",
    "\n",
    "# ===========================================================\n",
    "# String Normalization for Robust Matching\n",
    "# ===========================================================\n",
    "\n",
    "def normalize_string(s):\n",
    "     return s.strip().lower()\n",
    "\n",
    "# ===========================================================\n",
    "# Channel Filtering with Robust Matching\n",
    "# ===========================================================\n",
    "\n",
    "def filter_stream_channels_robust(stream, channel_names):\n",
    "\n",
    "    # Extract all available channel information from the stream\n",
    "    labels, types, units = extract_channel_labels(stream)\n",
    "\n",
    "    # Create a mapping from normalized names to original indices\n",
    "    # This allows O(1) lookup for each requested channel\n",
    "    normalized_labels = {normalize_string(label): i for i, label in enumerate(labels)}\n",
    "\n",
    "    # Normalize the requested channel names for comparison\n",
    "    normalized_requests = [normalize_string(name) for name in channel_names]\n",
    "\n",
    "    # Find indices of matching channels\n",
    "    matching_indices = []\n",
    "    matched_names = []\n",
    "    for req in normalized_requests:\n",
    "        if req in normalized_labels:\n",
    "            idx = normalized_labels[req]\n",
    "            matching_indices.append(idx)\n",
    "            matched_names.append(labels[idx])  # Keep original label (not normalized)\n",
    "\n",
    "    # Return None if no channels matched\n",
    "    if not matching_indices:\n",
    "        return None\n",
    "\n",
    "    # Create filtered stream with only the selected channels\n",
    "    filtered_stream = {\n",
    "        'info': stream['info'].copy(),  # Shallow copy of metadata\n",
    "        'time_series': stream['time_series'][:, matching_indices],  # Select columns\n",
    "        'time_stamps': stream.get('time_stamps', None),  # Preserve timestamps\n",
    "    }\n",
    "\n",
    "    # Update metadata to reflect the filtered channel set\n",
    "    try:\n",
    "        desc = filtered_stream['info']['desc'][0]\n",
    "        if 'channels' in desc and 'channel' in desc['channels'][0]:\n",
    "            original_channels = desc['channels'][0]['channel']\n",
    "            if isinstance(original_channels, list):\n",
    "                # Keep only the channel metadata entries that matched\n",
    "                filtered_channels = [original_channels[i] for i in matching_indices]\n",
    "                filtered_stream['info']['desc'][0]['channels'][0]['channel'] = filtered_channels\n",
    "        # Update the channel count in metadata\n",
    "        filtered_stream['info']['channel_count'] = [str(len(matching_indices))]\n",
    "    except Exception:\n",
    "        # If metadata update fails, continue (data is still filtered correctly)\n",
    "        pass\n",
    "\n",
    "    return filtered_stream\n",
    "\n",
    "# ===========================================================\n",
    "# Multi-Stream Selection with Diagnostics\n",
    "# ===========================================================\n",
    "\n",
    "def select_streams_by_name(streams, selection_dict, verbose=True):\n",
    "\n",
    "    selected_streams = []\n",
    "\n",
    "    # Create a lookup table: normalized name -> (original stream, original name)\n",
    "    stream_lookup = {}\n",
    "    for stream in streams:\n",
    "        stream_name = stream['info']['name'][0]\n",
    "        normalized_name = normalize_string(stream_name)\n",
    "        stream_lookup[normalized_name] = (stream, stream_name)\n",
    "\n",
    "    # Print available streams if verbose mode is enabled\n",
    "    if verbose:\n",
    "        print(f\"\\nüîç Available streams ({len(streams)}):\")\n",
    "        for norm_name, (_, orig_name) in stream_lookup.items():\n",
    "            print(f\"  '{orig_name}'\")\n",
    "        print()\n",
    "\n",
    "    # Process each requested stream\n",
    "    for requested_name, channels in selection_dict.items():\n",
    "        normalized_request = normalize_string(requested_name)\n",
    "\n",
    "        if \"polar\" in normalized_request:\n",
    "            # Search for a stream with \"Polar H10\" in original name (case-insensitive)\n",
    "            matching_stream = None\n",
    "            for norm_name, (stream, orig_name) in stream_lookup.items():\n",
    "                if \"polar h10\" in orig_name.lower():\n",
    "                    matching_stream = (stream, orig_name)\n",
    "                    break\n",
    "            if matching_stream:\n",
    "                stream, original_name = matching_stream\n",
    "                if verbose:\n",
    "                    print(f\"   Found stream matching 'Polar H10': '{original_name}'\")\n",
    "                filtered_stream = filter_stream_channels_robust(stream, channels)\n",
    "\n",
    "                if filtered_stream is not None:\n",
    "                    labels, _, _ = extract_channel_labels(filtered_stream)\n",
    "                    if verbose:\n",
    "                        print(f\"   Matched {len(labels)} channel(s): {labels}\")\n",
    "                    selected_streams.append(filtered_stream)\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print(f\"   No matching channels found in '{original_name}'\")\n",
    "                continue\n",
    "        # Check if the stream exists in our lookup\n",
    "        if normalized_request in stream_lookup:\n",
    "            stream, original_name = stream_lookup[normalized_request]\n",
    "            if verbose:\n",
    "                print(f\"   Found stream: '{original_name}'\")\n",
    "                print(f\"   Searching for channels: {channels}\")\n",
    "\n",
    "            # Filter the stream to include only requested channels\n",
    "            filtered_stream = filter_stream_channels_robust(stream, channels)\n",
    "\n",
    "            if filtered_stream is not None:\n",
    "                # Successfully found at least one matching channel\n",
    "                labels, _, _ = extract_channel_labels(filtered_stream)\n",
    "                if verbose:\n",
    "                    print(f\"   Matched {len(labels)} channel(s): {labels}\")\n",
    "                selected_streams.append(filtered_stream)\n",
    "            else:\n",
    "                # Stream was found, but none of the requested channels matched\n",
    "                if verbose:\n",
    "                    print(f\"   No matching channels found in '{original_name}'\")\n",
    "        else:\n",
    "            # Stream name didn't match any available streams\n",
    "            if verbose:\n",
    "                print(f\"Stream not found: '{requested_name}'\")\n",
    "                print(f\"   (normalized as: '{normalized_request}')\")\n",
    "\n",
    "    # Print summary\n",
    "    if verbose:\n",
    "        print(f\"\\n Total streams selected: {len(selected_streams)}\\n\")\n",
    "\n",
    "    return selected_streams\n",
    "\n",
    "# ===========================================================\n",
    "# Stream-to-DataFrame Conversion with Synchronization\n",
    "# ===========================================================\n",
    "\n",
    "def streams_to_dataframe(streams, resample=True, target_freq=1.0, use_timestamps=True, n=0):\n",
    "\n",
    "    # Handle empty input\n",
    "    if not streams:\n",
    "        print(\"No streams provided to streams_to_dataframe()\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    # Process each stream individually\n",
    "    for stream in streams:\n",
    "        # Extract stream metadata\n",
    "        name = stream['info']['name'][0]\n",
    "        data = stream['time_series']\n",
    "        ts = stream.get('time_stamps')\n",
    "        labels, _, _ = extract_channel_labels(stream)\n",
    "\n",
    "        # Safety check: ensure we have labels for all data columns\n",
    "        num_channels = data.shape[1]\n",
    "        labels = labels[:num_channels]\n",
    "\n",
    "        # Determine time column\n",
    "        if use_timestamps and ts is not None:\n",
    "            time_col = ts  # Use original timestamps\n",
    "        else:\n",
    "            # Generate synthetic timestamps assuming constant sampling rate\n",
    "            time_col = np.arange(len(data)) / target_freq\n",
    "\n",
    "        # Create DataFrame for this stream with prefixed column names\n",
    "        df_part = pd.DataFrame(data, columns=[f\"{name}_{label}\" for label in labels])\n",
    "        df_part.insert(0, \"XDFTime\", time_col)  # CHANGED: Rename to XDFTime\n",
    "        all_data.append(df_part)\n",
    "\n",
    "    # Determine the overlapping time range across all streams\n",
    "    # Use the latest start time and earliest end time\n",
    "    min_t = max(df[\"XDFTime\"].min() for df in all_data)  # CHANGED: XDFTime\n",
    "    max_t = min(df[\"XDFTime\"].max() for df in all_data)  # CHANGED: XDFTime\n",
    "\n",
    "    # Create the new time grid\n",
    "    if resample:\n",
    "        # Uniform grid at target_freq Hz\n",
    "        new_time = np.arange(min_t, max_t, 1.0 / target_freq)\n",
    "    else:\n",
    "        # Use the union of all original timestamps (sorted and unique)\n",
    "        new_time = sorted(set(np.concatenate([df[\"XDFTime\"].values for df in all_data])))  # CHANGED: XDFTime\n",
    "\n",
    "    # Initialize the merged data dictionary\n",
    "    merged = {\"XDFTime\": new_time}  # CHANGED: XDFTime\n",
    "\n",
    "    # Interpolate each stream's channels onto the new time grid\n",
    "    for df_part in all_data:\n",
    "        for col in df_part.columns:\n",
    "            if col == \"XDFTime\":  # CHANGED: XDFTime\n",
    "                continue  # Skip the time column itself\n",
    "\n",
    "            # Create interpolation function\n",
    "            f = interp1d(df_part[\"XDFTime\"], df_part[col],  # CHANGED: XDFTime\n",
    "                        fill_value=\"extrapolate\",\n",
    "                        bounds_error=False)\n",
    "\n",
    "            # Apply interpolation to new time grid\n",
    "            merged[col] = f(new_time)\n",
    "\n",
    "    # Create the final combined DataFrame\n",
    "    df = pd.DataFrame(merged)\n",
    "\n",
    "    # REMOVED: Time column generation here - we'll add it after truncation\n",
    "\n",
    "    # Truncate n samples from start and end if requested\n",
    "    if n > 0:\n",
    "        if len(df) > 2 * n:\n",
    "            # Remove first n and last n rows\n",
    "            df = df.iloc[n:-n].reset_index(drop=True)\n",
    "        else:\n",
    "            # Not enough data to truncate safely\n",
    "            print(f\"Warning: n={n} too large for dataset length {len(df)} ‚Äî skipping truncation.\")\n",
    "\n",
    "    # ADD Time column AFTER truncation to ensure it starts from 0\n",
    "    df.insert(0, \"Time\", np.arange(len(df)) / target_freq)\n",
    "\n",
    "    return df\n",
    "    \n",
    "# def streams_to_dataframe(streams, resample=True, target_freq=1.0, use_timestamps=True, n=0):\n",
    "\n",
    "#     # Handle empty input\n",
    "#     if not streams:\n",
    "#         print(\"No streams provided to streams_to_dataframe()\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     all_data = []\n",
    "\n",
    "#     # Process each stream individually\n",
    "#     for stream in streams:\n",
    "#         # Extract stream metadata\n",
    "#         name = stream['info']['name'][0]\n",
    "#         data = stream['time_series']\n",
    "#         ts = stream.get('time_stamps')\n",
    "#         labels, _, _ = extract_channel_labels(stream)\n",
    "\n",
    "#         # Safety check: ensure we have labels for all data columns\n",
    "#         num_channels = data.shape[1]\n",
    "#         labels = labels[:num_channels]\n",
    "\n",
    "#         # Determine time column\n",
    "#         if use_timestamps and ts is not None:\n",
    "#             time_col = ts  # Use original timestamps\n",
    "#         else:\n",
    "#             # Generate synthetic timestamps assuming constant sampling rate\n",
    "#             time_col = np.arange(len(data)) / target_freq\n",
    "\n",
    "#         # Create DataFrame for this stream with prefixed column names\n",
    "#         df_part = pd.DataFrame(data, columns=[f\"{name}_{label}\" for label in labels])\n",
    "#         df_part.insert(0, \"Time\", time_col)\n",
    "#         all_data.append(df_part)\n",
    "\n",
    "#     # Determine the overlapping time range across all streams\n",
    "#     # Use the latest start time and earliest end time\n",
    "#     min_t = max(df[\"Time\"].min() for df in all_data)\n",
    "#     max_t = min(df[\"Time\"].max() for df in all_data)\n",
    "\n",
    "#     # Create the new time grid\n",
    "#     if resample:\n",
    "#         # Uniform grid at target_freq Hz\n",
    "#         new_time = np.arange(min_t, max_t, 1.0 / target_freq)\n",
    "#     else:\n",
    "#         # Use the union of all original timestamps (sorted and unique)\n",
    "#         new_time = sorted(set(np.concatenate([df[\"Time\"].values for df in all_data])))\n",
    "\n",
    "#     # Initialize the merged data dictionary\n",
    "#     merged = {\"Time\": new_time}\n",
    "\n",
    "#     # Interpolate each stream's channels onto the new time grid\n",
    "#     for df_part in all_data:\n",
    "#         for col in df_part.columns:\n",
    "#             if col == \"Time\":\n",
    "#                 continue  # Skip the time column itself\n",
    "\n",
    "#             # Create interpolation function\n",
    "#             f = interp1d(df_part[\"Time\"], df_part[col],\n",
    "#                         fill_value=\"extrapolate\",\n",
    "#                         bounds_error=False)\n",
    "\n",
    "#             # Apply interpolation to new time grid\n",
    "#             merged[col] = f(new_time)\n",
    "\n",
    "#     # Create the final combined DataFrame\n",
    "#     df = pd.DataFrame(merged)\n",
    "\n",
    "#     # Truncate n samples from start and end if requested\n",
    "#     if n > 0:\n",
    "#         if len(df) > 2 * n:\n",
    "#             # Remove first n and last n rows\n",
    "#             df = df.iloc[n:-n].reset_index(drop=True)\n",
    "#         else:\n",
    "#             # Not enough data to truncate safely\n",
    "#             print(f\"Warning: n={n} too large for dataset length {len(df)} ‚Äî skipping truncation.\")\n",
    "\n",
    "#     return df\n",
    "\n",
    "# ===========================================================\n",
    "# Post-Processing: Resampling\n",
    "# ===========================================================\n",
    "\n",
    "def resample_dataframe(data, target_freq=1.0, verbose=True):\n",
    "\n",
    "    # Validate input - now check for XDFTime instead of Time\n",
    "    if \"XDFTime\" not in data.columns:  # CHANGED: XDFTime\n",
    "        raise ValueError(\"Input DataFrame must contain a 'XDFTime' column\")  # CHANGED: XDFTime\n",
    "\n",
    "    # Extract original time column (XDFTime)\n",
    "    original_time = data[\"XDFTime\"].values  # CHANGED: XDFTime\n",
    "\n",
    "    if len(original_time) < 2:\n",
    "        raise ValueError(\"DataFrame must have at least 2 time points for resampling\")\n",
    "\n",
    "    # Check original time spacing\n",
    "    time_diffs = np.diff(original_time)\n",
    "    tolerance = 1e-6\n",
    "    is_equally_spaced = np.allclose(time_diffs, np.mean(time_diffs), atol=tolerance)\n",
    "\n",
    "    if verbose:\n",
    "        if is_equally_spaced:\n",
    "            print(\"\\n‚úì Original time data is equally spaced.\")\n",
    "        else:\n",
    "            print(\"\\n‚ö† Original time data is NOT equally spaced.\")\n",
    "        print(f\"  Standard deviation of time differences: {np.std(time_diffs):.6f} seconds\")\n",
    "\n",
    "    # Create new uniform time grid for XDFTime\n",
    "    new_xdftime = np.arange(original_time[0], original_time[-1], 1.0 / target_freq)\n",
    "\n",
    "    # Helper function for interpolating a single column\n",
    "    def interpolate_column(original_time, col_data, new_time):\n",
    "        \"\"\"Interpolate a single data column onto a new time grid.\"\"\"\n",
    "        if len(original_time) < 2:\n",
    "            return np.full(len(new_time), np.mean(col_data))\n",
    "        else:\n",
    "            interp_func = interp1d(\n",
    "                original_time,\n",
    "                col_data,\n",
    "                kind='linear',\n",
    "                fill_value=\"extrapolate\",\n",
    "                bounds_error=False\n",
    "            )\n",
    "            return interp_func(new_time)\n",
    "\n",
    "    # Resample each data column (excluding both Time and XDFTime columns)\n",
    "    columns_to_resample = [col for col in data.columns if col not in [\"Time\", \"XDFTime\"]]  # CHANGED\n",
    "    resampled_data = {}\n",
    "\n",
    "    for col in columns_to_resample:\n",
    "        col_data = data[col].values\n",
    "        resampled_data[col] = interpolate_column(original_time, col_data, new_xdftime)\n",
    "\n",
    "    # Create final resampled DataFrame\n",
    "    final_dataset = pd.DataFrame(resampled_data)\n",
    "    final_dataset.insert(0, \"XDFTime\", new_xdftime)  # CHANGED: XDFTime\n",
    "    \n",
    "    # REMOVED: Time column generation here - we'll add it after truncation in the main pipeline\n",
    "\n",
    "    # Verify resampled time spacing for XDFTime\n",
    "    resampled_time_diffs = np.diff(new_xdftime)\n",
    "    is_resampled_equally_spaced = np.allclose(\n",
    "        resampled_time_diffs,\n",
    "        np.mean(resampled_time_diffs),\n",
    "        atol=tolerance\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        if is_resampled_equally_spaced:\n",
    "            print(\"\\n‚úì Resampled XDFTime data is equally spaced.\")  # CHANGED\n",
    "        else:\n",
    "            print(\"\\n‚ö† Resampled XDFTime data is NOT equally spaced.\")  # CHANGED\n",
    "        print(f\"  Standard deviation of resampled XDFTime differences: {np.std(resampled_time_diffs):.6e} seconds\")  # CHANGED\n",
    "\n",
    "        print(f\"\\nüìä Dataset Summary:\")\n",
    "        print(f\"  Shape: {final_dataset.shape} (rows √ó columns)\")\n",
    "        print(f\"  Total data points: {final_dataset.size:,}\")\n",
    "        print(f\"  XDFTime range: {new_xdftime[0]:.3f} - {new_xdftime[-1]:.3f} seconds\")  # CHANGED\n",
    "        print(f\"  Duration: {new_xdftime[-1] - new_xdftime[0]:.3f} seconds\")  # CHANGED\n",
    "        print(f\"  Sampling rate: {target_freq} Hz\")\n",
    "        print(f\"  Time step: {1.0/target_freq:.6f} seconds\")\n",
    "\n",
    "    return final_dataset\n",
    "    \n",
    "# def resample_dataframe(data, target_freq=1.0, verbose=True):\n",
    "\n",
    "#     # Validate input\n",
    "#     if \"Time\" not in data.columns:\n",
    "#         raise ValueError(\"Input DataFrame must contain a 'Time' column\")\n",
    "\n",
    "#     # Extract original time column\n",
    "#     original_time = data[\"Time\"].values\n",
    "\n",
    "#     if len(original_time) < 2:\n",
    "#         raise ValueError(\"DataFrame must have at least 2 time points for resampling\")\n",
    "\n",
    "#     # Check original time spacing\n",
    "#     time_diffs = np.diff(original_time)\n",
    "#     tolerance = 1e-6\n",
    "#     is_equally_spaced = np.allclose(time_diffs, np.mean(time_diffs), atol=tolerance)\n",
    "\n",
    "#     if verbose:\n",
    "#         if is_equally_spaced:\n",
    "#             print(\"\\n‚úì Original time data is equally spaced.\")\n",
    "#         else:\n",
    "#             print(\"\\n‚ö† Original time data is NOT equally spaced.\")\n",
    "#         print(f\"  Standard deviation of time differences: {np.std(time_diffs):.6f} seconds\")\n",
    "\n",
    "#     # Create new uniform time grid\n",
    "#     new_time = np.arange(original_time[0], original_time[-1], 1.0 / target_freq)\n",
    "\n",
    "#     # Helper function for interpolating a single column\n",
    "#     def interpolate_column(original_time, col_data, new_time):\n",
    "#         \"\"\"Interpolate a single data column onto a new time grid.\"\"\"\n",
    "#         if len(original_time) < 2:\n",
    "#             return np.full(len(new_time), np.mean(col_data))\n",
    "#         else:\n",
    "#             interp_func = interp1d(\n",
    "#                 original_time,\n",
    "#                 col_data,\n",
    "#                 kind='linear',\n",
    "#                 fill_value=\"extrapolate\",\n",
    "#                 bounds_error=False\n",
    "#             )\n",
    "#             return interp_func(new_time)\n",
    "\n",
    "#     # Resample each data column\n",
    "#     columns_to_resample = [col for col in data.columns if col != \"Time\"]\n",
    "#     resampled_data = {}\n",
    "\n",
    "#     for col in columns_to_resample:\n",
    "#         col_data = data[col].values\n",
    "#         resampled_data[col] = interpolate_column(original_time, col_data, new_time)\n",
    "\n",
    "#     # Create final resampled DataFrame\n",
    "#     final_dataset = pd.DataFrame(resampled_data)\n",
    "#     final_dataset.insert(0, \"Time\", new_time)\n",
    "\n",
    "#     # Verify resampled time spacing\n",
    "#     resampled_time_diffs = np.diff(new_time)\n",
    "#     is_resampled_equally_spaced = np.allclose(\n",
    "#         resampled_time_diffs,\n",
    "#         np.mean(resampled_time_diffs),\n",
    "#         atol=tolerance\n",
    "#     )\n",
    "\n",
    "#     if verbose:\n",
    "#         if is_resampled_equally_spaced:\n",
    "#             print(\"\\n‚úì Resampled time data is equally spaced.\")\n",
    "#         else:\n",
    "#             print(\"\\n‚ö† Resampled time data is NOT equally spaced.\")\n",
    "#         print(f\"  Standard deviation of resampled time differences: {np.std(resampled_time_diffs):.6e} seconds\")\n",
    "\n",
    "#         print(f\"\\nüìä Dataset Summary:\")\n",
    "#         print(f\"  Shape: {final_dataset.shape} (rows √ó columns)\")\n",
    "#         print(f\"  Total data points: {final_dataset.size:,}\")\n",
    "#         print(f\"  Time range: {new_time[0]:.3f} - {new_time[-1]:.3f} seconds\")\n",
    "#         print(f\"  Duration: {new_time[-1] - new_time[0]:.3f} seconds\")\n",
    "#         print(f\"  Sampling rate: {target_freq} Hz\")\n",
    "#         print(f\"  Time step: {1.0/target_freq:.6f} seconds\")\n",
    "\n",
    "#     return final_dataset\n",
    "\n",
    "# ===========================================================\n",
    "# Post-Processing: Truncation\n",
    "# ===========================================================\n",
    "\n",
    "def truncate_dataframe(data, n=0, verbose=True):\n",
    "\n",
    "    if n < 0:\n",
    "        raise ValueError(f\"n must be non-negative, got n={n}\")\n",
    "\n",
    "    if n == 0:\n",
    "        if verbose:\n",
    "            print(\"\\n‚Ñπ No truncation performed (n=0)\")\n",
    "        return data\n",
    "\n",
    "    original_length = len(data)\n",
    "\n",
    "    if original_length <= 2 * n:\n",
    "        if verbose:\n",
    "            print(f\"\\n‚ö† Warning: n={n} is too large for dataset length {original_length} ‚Äî skipping truncation.\")\n",
    "            print(f\"   Minimum length required: {2*n + 1} (to remove {n} from each end)\")\n",
    "        return data\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nüìä Truncation Summary:\")\n",
    "        print(f\"  Original shape: {data.shape}\")\n",
    "        print(f\"  Removing {n} rows from start and end ({2*n} total)\")\n",
    "\n",
    "    truncated_data = data.iloc[n:-n].reset_index(drop=True)\n",
    "    \n",
    "    # REGENERATE Time column to ensure it starts from 0 after truncation\n",
    "    if \"Time\" in truncated_data.columns:\n",
    "        # Remove old Time column\n",
    "        truncated_data = truncated_data.drop(columns=[\"Time\"])\n",
    "    # Add new Time column starting from 0\n",
    "    target_freq = 1.0 / (truncated_data[\"XDFTime\"].iloc[1] - truncated_data[\"XDFTime\"].iloc[0]) if len(truncated_data) > 1 else 1.0\n",
    "    truncated_data.insert(0, \"Time\", np.arange(len(truncated_data)) / target_freq)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"  Final shape: {truncated_data.shape}\")\n",
    "\n",
    "        if \"XDFTime\" in truncated_data.columns:\n",
    "            time_removed_start = data.iloc[n][\"XDFTime\"] - data.iloc[0][\"XDFTime\"]\n",
    "            time_removed_end = data.iloc[-1][\"XDFTime\"] - data.iloc[-n-1][\"XDFTime\"]\n",
    "            print(f\"  XDFTime removed from start: {time_removed_start:.3f} seconds\")\n",
    "            print(f\"  XDFTime removed from end: {time_removed_end:.3f} seconds\")\n",
    "            print(f\"  New XDFTime range: {truncated_data['XDFTime'].iloc[0]:.3f} - {truncated_data['XDFTime'].iloc[-1]:.3f} seconds\")\n",
    "            print(f\"  New Time range: {truncated_data['Time'].iloc[0]:.3f} - {truncated_data['Time'].iloc[-1]:.3f} seconds\")  # Now starts at 0!\n",
    "\n",
    "    return truncated_data\n",
    "    \n",
    "# def truncate_dataframe(data, n=0, verbose=True):\n",
    "\n",
    "#     if n < 0:\n",
    "#         raise ValueError(f\"n must be non-negative, got n={n}\")\n",
    "\n",
    "#     if n == 0:\n",
    "#         if verbose:\n",
    "#             print(\"\\n‚Ñπ No truncation performed (n=0)\")\n",
    "#         return data\n",
    "\n",
    "#     original_length = len(data)\n",
    "\n",
    "#     if original_length <= 2 * n:\n",
    "#         if verbose:\n",
    "#             print(f\"\\n‚ö† Warning: n={n} is too large for dataset length {original_length} ‚Äî skipping truncation.\")\n",
    "#             print(f\"   Minimum length required: {2*n + 1} (to remove {n} from each end)\")\n",
    "#         return data\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"\\nüìä Truncation Summary:\")\n",
    "#         print(f\"  Original shape: {data.shape}\")\n",
    "#         print(f\"  Removing {n} rows from start and end ({2*n} total)\")\n",
    "\n",
    "#     truncated_data = data.iloc[n:-n].reset_index(drop=True)\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"  Final shape: {truncated_data.shape}\")\n",
    "\n",
    "#         if \"Time\" in truncated_data.columns:\n",
    "#             time_removed_start = data.iloc[n][\"Time\"] - data.iloc[0][\"Time\"]\n",
    "#             time_removed_end = data.iloc[-1][\"Time\"] - data.iloc[-n-1][\"Time\"]\n",
    "#             print(f\"  Time removed from start: {time_removed_start:.3f} seconds\")\n",
    "#             print(f\"  Time removed from end: {time_removed_end:.3f} seconds\")\n",
    "#             print(f\"  New time range: {truncated_data['Time'].iloc[0]:.3f} - {truncated_data['Time'].iloc[-1]:.3f} seconds\")\n",
    "\n",
    "#     return truncated_data\n",
    "\n",
    "# ===========================================================\n",
    "# Diagnostic and Utility Functions\n",
    "# ===========================================================\n",
    "\n",
    "def print_stream_info(streams):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Found {len(streams)} stream(s):\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    for stream in streams:\n",
    "        name = stream['info']['name'][0]\n",
    "        labels, types, units = extract_channel_labels(stream)\n",
    "\n",
    "        print(f\" Stream: {name}\")\n",
    "        print(f\"   Channels ({len(labels)}):\")\n",
    "\n",
    "        for l, t, u in zip(labels, types, units):\n",
    "            type_str = f\"({t})\" if t else \"\"\n",
    "            unit_str = f\"[{u}]\" if u else \"\"\n",
    "            print(f\"      ‚Ä¢ {l} {type_str} {unit_str}\".strip())\n",
    "\n",
    "        print(f\"{'-'*60}\\n\")\n",
    "\n",
    "\n",
    "def get_copyable_format(streams):\n",
    "    print(\"\\n Copy this template and fill in your desired channels:\\n\")\n",
    "    print(\"selection_dict = {\")\n",
    "\n",
    "    for stream in streams:\n",
    "        name = stream['info']['name'][0]\n",
    "        labels, _, _ = extract_channel_labels(stream)\n",
    "        print(f'    \"{name}\": {labels},')\n",
    "\n",
    "    print(\"}\\n\")\n",
    "\n",
    "# ===========================================================\n",
    "# Complete Pipeline: All-in-One Processing\n",
    "# ===========================================================\n",
    "\n",
    "def process_xdf_streams(streams, selection_dict, target_freq=50.0, truncate_n=0, verbose=True):\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"XDF STREAM PROCESSING PIPELINE\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nüìã Configuration:\")\n",
    "        print(f\"  Target frequency: {target_freq} Hz\")\n",
    "        print(f\"  Truncation: {truncate_n} samples from each end\")\n",
    "        print(f\"  Streams requested: {len(selection_dict)}\")\n",
    "\n",
    "    # STEP 1: Stream Selection\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(\"STEP 1: Stream Selection\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "    selected_streams = select_streams_by_name(streams, selection_dict, verbose=verbose)\n",
    "\n",
    "    if not selected_streams:\n",
    "        if verbose:\n",
    "            print(\"\\n‚ùå No streams selected. Check your selection_dict.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # STEP 2: Synchronization and Initial Processing\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(\"STEP 2: Synchronization and Resampling\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "    df = streams_to_dataframe(\n",
    "        selected_streams,\n",
    "        resample=True,\n",
    "        target_freq=target_freq,\n",
    "        use_timestamps=True,\n",
    "        n=0\n",
    "    )\n",
    "\n",
    "    if df.empty:\n",
    "        if verbose:\n",
    "            print(\"\\n‚ùå Empty DataFrame after synchronization.\")\n",
    "        return df\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n‚úì Synchronized DataFrame created\")\n",
    "        print(f\"  Shape: {df.shape}\")\n",
    "        print(f\"  Time range: {df['Time'].iloc[0]:.3f} - {df['Time'].iloc[-1]:.3f} seconds\")\n",
    "        print(f\"  Duration: {df['Time'].iloc[-1] - df['Time'].iloc[0]:.3f} seconds\")\n",
    "\n",
    "    # STEP 3: Final Resampling\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(\"STEP 3: Final Resampling Verification\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "    df_resampled = resample_dataframe(df, target_freq=target_freq, verbose=verbose)\n",
    "\n",
    "    # STEP 4: Truncation\n",
    "    if truncate_n > 0:\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"-\"*70)\n",
    "            print(\"STEP 4: Edge Truncation\")\n",
    "            print(\"-\"*70)\n",
    "\n",
    "        df_final = truncate_dataframe(df_resampled, n=truncate_n, verbose=verbose)\n",
    "    else:\n",
    "        df_final = df_resampled\n",
    "        # Ensure Time column starts from 0 even when no truncation\n",
    "        if \"Time\" in df_final.columns:\n",
    "            df_final = df_final.drop(columns=[\"Time\"])\n",
    "        df_final.insert(0, \"Time\", np.arange(len(df_final)) / target_freq)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"-\"*70)\n",
    "            print(\"STEP 4: Edge Truncation\")\n",
    "            print(\"-\"*70)\n",
    "            print(\"\\n‚Ñπ Truncation skipped (truncate_n=0)\")\n",
    "\n",
    "    # Final Summary\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PROCESSING COMPLETE ‚úì\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nüìä Final Dataset Summary:\")\n",
    "        print(f\"  Shape: {df_final.shape[0]:,} rows √ó {df_final.shape[1]} columns\")\n",
    "        print(f\"  Time range: {df_final['Time'].iloc[0]:.3f} - {df_final['Time'].iloc[-1]:.3f} seconds\")  # Now always starts at 0!\n",
    "        print(f\"  Duration: {df_final['Time'].iloc[-1] - df_final['Time'].iloc[0]:.3f} seconds\")\n",
    "        print(f\"  Sampling frequency: {target_freq} Hz\")\n",
    "        print(f\"  Columns: {list(df_final.columns)}\")\n",
    "        print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "    return df_final\n",
    "    \n",
    "# def process_xdf_streams(streams, selection_dict, target_freq=50.0, truncate_n=0, verbose=True):\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"\\n\" + \"=\"*70)\n",
    "#         print(\"XDF STREAM PROCESSING PIPELINE\")\n",
    "#         print(\"=\"*70)\n",
    "#         print(\"\\nüìã Configuration:\")\n",
    "#         print(f\"  Target frequency: {target_freq} Hz\")\n",
    "#         print(f\"  Truncation: {truncate_n} samples from each end\")\n",
    "#         print(f\"  Streams requested: {len(selection_dict)}\")\n",
    "\n",
    "#     # STEP 1: Stream Selection\n",
    "#     if verbose:\n",
    "#         print(\"\\n\" + \"-\"*70)\n",
    "#         print(\"STEP 1: Stream Selection\")\n",
    "#         print(\"-\"*70)\n",
    "\n",
    "#     selected_streams = select_streams_by_name(streams, selection_dict, verbose=verbose)\n",
    "\n",
    "#     if not selected_streams:\n",
    "#         if verbose:\n",
    "#             print(\"\\n‚ùå No streams selected. Check your selection_dict.\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     # STEP 2: Synchronization and Initial Processing\n",
    "#     if verbose:\n",
    "#         print(\"\\n\" + \"-\"*70)\n",
    "#         print(\"STEP 2: Synchronization and Resampling\")\n",
    "#         print(\"-\"*70)\n",
    "\n",
    "#     df = streams_to_dataframe(\n",
    "#         selected_streams,\n",
    "#         resample=True,\n",
    "#         target_freq=target_freq,\n",
    "#         use_timestamps=True,\n",
    "#         n=0\n",
    "#     )\n",
    "\n",
    "#     if df.empty:\n",
    "#         if verbose:\n",
    "#             print(\"\\n‚ùå Empty DataFrame after synchronization.\")\n",
    "#         return df\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"\\n‚úì Synchronized DataFrame created\")\n",
    "#         print(f\"  Shape: {df.shape}\")\n",
    "#         print(f\"  Time range: {df['Time'].iloc[0]:.3f} - {df['Time'].iloc[-1]:.3f} seconds\")\n",
    "#         print(f\"  Duration: {df['Time'].iloc[-1] - df['Time'].iloc[0]:.3f} seconds\")\n",
    "\n",
    "#     # STEP 3: Final Resampling\n",
    "#     if verbose:\n",
    "#         print(\"\\n\" + \"-\"*70)\n",
    "#         print(\"STEP 3: Final Resampling Verification\")\n",
    "#         print(\"-\"*70)\n",
    "\n",
    "#     df_resampled = resample_dataframe(df, target_freq=target_freq, verbose=verbose)\n",
    "\n",
    "#     # STEP 4: Truncation\n",
    "#     if truncate_n > 0:\n",
    "#         if verbose:\n",
    "#             print(\"\\n\" + \"-\"*70)\n",
    "#             print(\"STEP 4: Edge Truncation\")\n",
    "#             print(\"-\"*70)\n",
    "\n",
    "#         df_final = truncate_dataframe(df_resampled, n=truncate_n, verbose=verbose)\n",
    "#     else:\n",
    "#         df_final = df_resampled\n",
    "#         if verbose:\n",
    "#             print(\"\\n\" + \"-\"*70)\n",
    "#             print(\"STEP 4: Edge Truncation\")\n",
    "#             print(\"-\"*70)\n",
    "#             print(\"\\n‚Ñπ Truncation skipped (truncate_n=0)\")\n",
    "\n",
    "#     # Final Summary\n",
    "#     if verbose:\n",
    "#         print(\"\\n\" + \"=\"*70)\n",
    "#         print(\"PROCESSING COMPLETE ‚úì\")\n",
    "#         print(\"=\"*70)\n",
    "#         print(f\"\\nüìä Final Dataset Summary:\")\n",
    "#         print(f\"  Shape: {df_final.shape[0]:,} rows √ó {df_final.shape[1]} columns\")\n",
    "#         print(f\"  Time range: {df_final['Time'].iloc[0]:.3f} - {df_final['Time'].iloc[-1]:.3f} seconds\")\n",
    "#         print(f\"  Duration: {df_final['Time'].iloc[-1] - df_final['Time'].iloc[0]:.3f} seconds\")\n",
    "#         print(f\"  Sampling frequency: {target_freq} Hz\")\n",
    "#         print(f\"  Columns: {list(df_final.columns)}\")\n",
    "#         print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "#     return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZRKW5tX4TT6f",
   "metadata": {
    "id": "ZRKW5tX4TT6f"
   },
   "source": [
    "# Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jAe_tSpYTKIO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "jAe_tSpYTKIO",
    "outputId": "010c6ab8-fdae-4813-acce-963fbe89d4cf"
   },
   "outputs": [],
   "source": [
    "## Data Upload\n",
    "\n",
    "# Read CSV directly with full path\n",
    "directory = r'placeholder' # Data Directory\n",
    "df = pd.read_csv(directory)\n",
    "print(f\"Original data shape: {df.shape}\")\n",
    "survey_save_to = r'placeholder' # Save Directory\n",
    "\n",
    "# # Display available columns with indices\n",
    "# print(\"Available columns:\")\n",
    "# for i, col in enumerate(df.columns):\n",
    "#     print(f\"  ({i}) {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eZm4_BImVdCx",
   "metadata": {
    "id": "eZm4_BImVdCx"
   },
   "outputs": [],
   "source": [
    "# # Quick one-liner for checking column index\n",
    "# for idx, col in enumerate(df.columns):\n",
    "#     print(f\"[{idx}] {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RzaA-rVfTWw5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RzaA-rVfTWw5",
    "outputId": "1118f8c8-fb43-4143-c4e1-a447c1b7643c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select Columns and Rows\n",
    "selected_columns = df.columns[list(range(18, 35)) + list(range(35, 51))]\n",
    "selected_rows = slice(-34, None)\n",
    "\n",
    "# Create final DataFrame\n",
    "selected_data = df[selected_columns].iloc[selected_rows]\n",
    "\n",
    "# Show result\n",
    "print(f\"\\nSelected {len(selected_columns)} columns and {len(selected_data)} rows\")\n",
    "print(f\"Final shape: {selected_data.shape}\")\n",
    "print(\"\\nFinal DataFrame:\")\n",
    "display(selected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HajzsPM1_4OL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HajzsPM1_4OL",
    "outputId": "5a3a136f-b105-4b4e-fe49-db3459a6a7f0"
   },
   "outputs": [],
   "source": [
    "## Survey Data Cleanup\n",
    "\n",
    "# Make a proper copy first to avoid SettingWithCopyWarning\n",
    "selected_data = selected_data.copy()\n",
    "\n",
    "# Clean up the 'pid' column, Remove '#' from values like '#5'\n",
    "selected_data['pid'] = selected_data['pid'].astype(str).str.replace('#', '', regex=False)\n",
    "\n",
    "# Drop rows where pid starts with an alphabet, Keep only rows where pid starts with a digit (after removing #)\n",
    "selected_data = selected_data[selected_data['pid'].str.match(r'^\\d', na=False)].copy()\n",
    "\n",
    "# Remove any comma from the 'flt_hrs' column\n",
    "selected_data['flt_hrs'] = selected_data['flt_hrs'].astype(str).str.replace(',', '', regex=False)\n",
    "\n",
    "# Clean up the 'experience' column\n",
    "# Convert to string first\n",
    "selected_data['experience'] = selected_data['experience'].astype(str)\n",
    "\n",
    "# For values that look like years (>= 1900), calculate experience as 2025 - year\n",
    "# For normal experience values, keep as is\n",
    "def clean_experience(val):\n",
    "    try:\n",
    "        num = float(val.replace(',', ''))  # Remove commas if any\n",
    "        if num >= 1900:  # Likely a year\n",
    "            return str(int(2025 - num))\n",
    "        else:  # Already an experience value\n",
    "            return str(int(num))\n",
    "    except:\n",
    "        return val  # Keep original if conversion fails\n",
    "\n",
    "selected_data['experience'] = selected_data['experience'].apply(clean_experience)\n",
    "\n",
    "# Rename the cleaned data\n",
    "clean_survey_data = selected_data.copy()\n",
    "\n",
    "print(f\"\\nAfter cleaning pid, flt_hrs, and experience columns:\")\n",
    "print(f\"Final shape: {clean_survey_data.shape}\")\n",
    "print(f\"\\nCleaned pid values:\")\n",
    "print(clean_survey_data['pid'].unique())\n",
    "print(f\"\\nCleaned flt_hrs sample:\")\n",
    "print(clean_survey_data['flt_hrs'].head(10))\n",
    "print(f\"\\nCleaned experience values:\")\n",
    "print(clean_survey_data['experience'].unique())\n",
    "\n",
    "# Save as CSV\n",
    "clean_survey_data.to_csv(os.path.join(survey_save_to, 'clean_survey_data.csv'), index=False)\n",
    "print(f\"\\n‚úì Saved to: {os.path.join(survey_save_to, 'clean_survey_data.csv')}\")\n",
    "display(clean_survey_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xAWXJfTMOteY",
   "metadata": {
    "id": "xAWXJfTMOteY"
   },
   "outputs": [],
   "source": [
    "# # Quick one-liner for checking column index\n",
    "# for idx, col in enumerate(clean_survey_data.columns):\n",
    "#     print(f\"[{idx}] {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z45Ycue2h8Rv",
   "metadata": {
    "id": "Z45Ycue2h8Rv"
   },
   "source": [
    "# XDF Test (Single File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7767ad-e4c0-4795-ab85-04b45ca492d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = r'placeholder'\n",
    "# Load Data\n",
    "streams, header = pyxdf.load_xdf(file_path)\n",
    "test_save = r'placeholder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e622ec5-8118-4917-890a-1d1802018083",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_channel_labels(stream):\n",
    "    \"\"\"\n",
    "    Extracts channel labels from a stream, handling multiple metadata formats.\n",
    "    Returns a list of labels and optionally types and units if available.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        channel_entries = stream['info']['desc'][0]['channels'][0]['channel']\n",
    "        if not isinstance(channel_entries, list):\n",
    "            channel_entries = [channel_entries]\n",
    "\n",
    "        labels = []\n",
    "        types = []\n",
    "        units = []\n",
    "\n",
    "        for i, ch in enumerate(channel_entries):\n",
    "            label = ch.get('label', ch.get('name', [f\"Channel {i}\"]))[0]\n",
    "            signal_type = ch.get('type', [''])[0]\n",
    "            unit = ch.get('unit', [''])[0]\n",
    "\n",
    "            labels.append(label)\n",
    "            types.append(signal_type)\n",
    "            units.append(unit)\n",
    "\n",
    "        return labels, types, units\n",
    "\n",
    "    except (KeyError, IndexError, TypeError):\n",
    "        # Fallback: use generic channel names\n",
    "        num_channels = stream['info'].get('channel_count', ['0'])[0]\n",
    "        try:\n",
    "            num_channels = int(num_channels)\n",
    "        except ValueError:\n",
    "            num_channels = 1\n",
    "        labels = [f\"Channel {i}\" for i in range(num_channels)]\n",
    "        return labels, [''] * num_channels, [''] * num_channels\n",
    "\n",
    "# View stream names and channel details\n",
    "print(\"=== Streams and Channels ===\")\n",
    "print(f\"Number of streams: {len(streams)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, stream in enumerate(streams):\n",
    "    stream_name = stream['info']['name'][0]\n",
    "    num_channels = int(stream['info']['channel_count'][0])\n",
    "    labels, types, units = extract_channel_labels(stream)\n",
    "    \n",
    "    print(f\"Stream {i}: '{stream_name}' with {num_channels} channels\")\n",
    "    print(\"Channel details:\")\n",
    "    for j in range(len(labels)):\n",
    "        type_info = f\" ({types[j]})\" if types[j] else \"\"\n",
    "        unit_info = f\" [{units[j]}]\" if units[j] else \"\"\n",
    "        print(f\"  {j}: {labels[j]}{type_info}{unit_info}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# # Print full metadata for the first stream\n",
    "# print(\"\\n=== Full Metadata for First Stream ===\")\n",
    "# pprint.pprint(streams[0]['info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d6b0a-52c6-46f1-a295-aac98ad768e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = {\n",
    "    \"VarjoEyeMetrics\": [\n",
    "        \"focusDistance\",\n",
    "        \"stability\", \n",
    "        \"interPupillaryDist\",\n",
    "        \"leftPupilIrisRatio\",\n",
    "        \"rightPupilIrisRatio\",\n",
    "        \"leftPupilDiam\",\n",
    "        \"rightPupilDiam\", \n",
    "        \"leftIrisDiam\",\n",
    "        \"rightIrisDiam\",\n",
    "        \"leftOpenness\",\n",
    "        \"rightOpenness\"\n",
    "    ],\n",
    "    \"VarjoGaze\": [\n",
    "        \"fwdX\", \n",
    "        \"fwdY\", \n",
    "        \"fwdZ\"\n",
    "    ],\n",
    "    \"XPlaneData\": [\n",
    "        \"sim/flightmodel/position/latitude\",\n",
    "        \"sim/flightmodel/position/longitude\",\n",
    "        \"sim/cockpit2/gauges/indicators/altitude_ft_pilot\",\n",
    "        \"sim/cockpit/autopilot/airspeed\"\n",
    "    ],\n",
    "    \"Polar H10\": [\n",
    "        \"HR\",\n",
    "        \"RRI\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define column name mappings\n",
    "column_rename_map = {\n",
    "    # VarjoEyeMetrics\n",
    "    \"VarjoEyeMetrics_focusDistance\": \"Focus Distance\",\n",
    "    \"VarjoEyeMetrics_stability\": \"Gaze Stability\",\n",
    "    \"VarjoEyeMetrics_interPupillaryDist\": \"Interpupillary Distance\",\n",
    "    \"VarjoEyeMetrics_leftPupilIrisRatio\": \"Left Pupil Iris Ratio\",\n",
    "    \"VarjoEyeMetrics_rightPupilIrisRatio\": \"Right Pupil Iris Ratio\",\n",
    "    \"VarjoEyeMetrics_leftPupilDiam\": \"Left Pupil Diameter\",\n",
    "    \"VarjoEyeMetrics_rightPupilDiam\": \"Right Pupil Diameter\",\n",
    "    \"VarjoEyeMetrics_leftIrisDiam\": \"Left Iris Diameter\",\n",
    "    \"VarjoEyeMetrics_rightIrisDiam\": \"Right Iris Diameter\",\n",
    "    \"VarjoEyeMetrics_leftOpenness\": \"Left Eye Openness\",\n",
    "    \"VarjoEyeMetrics_rightOpenness\": \"Right Eye Openness\",\n",
    "    \n",
    "    # VarjoGaze\n",
    "    \"VarjoGaze_fwdX\": \"Gaze Forward X\",\n",
    "    \"VarjoGaze_fwdY\": \"Gaze Forward Y\", \n",
    "    \"VarjoGaze_fwdZ\": \"Gaze Forward Z\",\n",
    "    \n",
    "    # XPlaneData (simplified names)\n",
    "    \"XPlaneData_sim/flightmodel/position/latitude\": \"Latitude\",\n",
    "    \"XPlaneData_sim/flightmodel/position/longitude\": \"Longitude\",\n",
    "    \"XPlaneData_sim/cockpit2/gauges/indicators/altitude_ft_pilot\": \"Altitude\",\n",
    "    \"XPlaneData_sim/cockpit/autopilot/airspeed\": \"Airspeed\",\n",
    "    \n",
    "    # Polar H10 - these will be dynamically matched\n",
    "    \"HR\": \"Heart Rate\",\n",
    "    \"RRI\": \"R-R Interval\"\n",
    "}\n",
    "\n",
    "# Function to rename Polar columns dynamically\n",
    "def rename_polar_columns(df):\n",
    "    \"\"\"Rename any columns containing 'Polar' to standardized names\"\"\"\n",
    "    new_columns = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if 'Polar' in col:\n",
    "            # Extract the actual measurement name (after the last underscore)\n",
    "            measurement = col.split('_')[-1] if '_' in col else col\n",
    "            \n",
    "            # Map to standardized name\n",
    "            if measurement in column_rename_map:\n",
    "                new_columns[col] = column_rename_map[measurement]\n",
    "            else:\n",
    "                # Keep original but clean it up\n",
    "                new_columns[col] = f\"Polar {measurement}\"\n",
    "    \n",
    "    return df.rename(columns=new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9b93b-275e-45d4-a5b4-3789e3c130d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Run the complete pipeline for a single file\n",
    "    df_complete = process_xdf_streams(\n",
    "        streams=streams,\n",
    "        selection_dict=selection,\n",
    "        target_freq=4,\n",
    "        truncate_n=20,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # First rename non-Polar columns using the static mapping\n",
    "    df_complete = df_complete.rename(columns=column_rename_map)\n",
    "    \n",
    "    # Then dynamically rename any Polar columns\n",
    "    df_complete = rename_polar_columns(df_complete)\n",
    "    \n",
    "    # Display the resulting dataframe\n",
    "    print(\"Processed Dataframe:\")\n",
    "    display(df_complete)\n",
    "    print(f\"Data shape: {df_complete.shape}\")\n",
    "    print(f\"Data columns: {df_complete.columns.tolist()}\")\n",
    "\n",
    "    # Create CSV filename in the test_save directory\n",
    "    import os\n",
    "    filename = os.path.basename(file_path).replace('.xdf', '.csv')\n",
    "    csv_filename = os.path.join(test_save, filename)\n",
    "    df_complete.to_csv(csv_filename, index=False)\n",
    "    print(f\"‚úì Saved processed data to: {csv_filename}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during processing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkLn2DHfDS-H",
   "metadata": {
    "id": "bkLn2DHfDS-H"
   },
   "source": [
    "# XDF b1: Baseline HRV (Multiple Files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d116701e-3d5c-44f0-bc8b-9ce308cec1e8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4C44rFiQiT5O",
    "outputId": "49b98a4b-71ae-491d-a928-e662a9313b35"
   },
   "outputs": [],
   "source": [
    "# Define Directory, Selection and Dataframe Names\n",
    "directory = r'placeholder' # Data Directory\n",
    "selection = {\n",
    "    \"Polar H10\": [\"HR\", \"RRI\"]\n",
    "}\n",
    "hrv_save_to = r'placeholder' # Save Directory\n",
    "\n",
    "# Define column name mappings for Polar data\n",
    "column_rename_map = {\n",
    "    \"HR\": \"Heart Rate\",\n",
    "    \"RRI\": \"R-R Interval\"\n",
    "}\n",
    "\n",
    "# Function to rename Polar columns dynamically\n",
    "def rename_polar_columns(df):\n",
    "    \"\"\"Rename any columns containing 'Polar' to standardized names\"\"\"\n",
    "    new_columns = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if 'Polar' in col:\n",
    "            # Extract the actual measurement name (after the last underscore)\n",
    "            measurement = col.split('_')[-1] if '_' in col else col\n",
    "            \n",
    "            # Map to standardized name\n",
    "            if measurement in column_rename_map:\n",
    "                new_columns[col] = column_rename_map[measurement]\n",
    "            else:\n",
    "                # Keep original but clean it up\n",
    "                new_columns[col] = f\"Polar {measurement}\"\n",
    "        else:\n",
    "            # Keep non-Polar columns as they are\n",
    "            new_columns[col] = col\n",
    "    \n",
    "    return df.rename(columns=new_columns)\n",
    "\n",
    "# Loop through all .xdf files and create the .csv dataframe by the same name\n",
    "files = os.listdir(directory)\n",
    "for file in files:\n",
    "    if file.endswith('.xdf'):\n",
    "        # Create full path to the file\n",
    "        filepath = os.path.join(directory, file)\n",
    "\n",
    "        try:\n",
    "            streams, header = pyxdf.load_xdf(filepath)  # Use filepath instead of file\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df_complete = process_xdf_streams(\n",
    "                streams=streams,\n",
    "                selection_dict=selection,\n",
    "                target_freq=4.0,\n",
    "                truncate_n=20,\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            # Rename Polar columns dynamically\n",
    "            df_complete = rename_polar_columns(df_complete)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process streams from {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Create savename by replacing .xdf extension with .csv\n",
    "        savename = file.replace('.xdf', '.csv')\n",
    "        # Save to the hrv_save_to directory\n",
    "        savepath = os.path.join(hrv_save_to, savename)\n",
    "\n",
    "        try:\n",
    "            df_complete.to_csv(savepath, index=False)  # Save to full path\n",
    "            print(f\"Saved {savename} with {len(df_complete)} rows to {hrv_save_to}\")\n",
    "            print(f\"Columns: {list(df_complete.columns)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save {savename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd20ed36-8ed8-466b-b027-1320d4795100",
   "metadata": {},
   "source": [
    "# XDF b3: Station B (Multiple Files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d93bc3-c301-444f-8f9a-aca86df6210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b3 = Station B\n",
    "\n",
    "# Define Directory, Selection and Dataframe Names\n",
    "directory = r'placeholder' # b3 =Station B\n",
    "selection = {\n",
    "    \"VarjoEyeMetrics\": [\n",
    "        \"focusDistance\",\n",
    "        \"stability\", \n",
    "        \"interPupillaryDist\",\n",
    "        \"leftPupilIrisRatio\",\n",
    "        \"rightPupilIrisRatio\",\n",
    "        \"leftPupilDiam\",\n",
    "        \"rightPupilDiam\", \n",
    "        \"leftIrisDiam\",\n",
    "        \"rightIrisDiam\",\n",
    "        \"leftOpenness\",\n",
    "        \"rightOpenness\"\n",
    "    ],\n",
    "    \"VarjoGaze\": [\n",
    "        \"fwdX\", \n",
    "        \"fwdY\", \n",
    "        \"fwdZ\"\n",
    "    ],\n",
    "    \"XPlaneData\": [\n",
    "        \"sim/flightmodel/position/latitude\",\n",
    "        \"sim/flightmodel/position/longitude\",\n",
    "        \"sim/cockpit2/gauges/indicators/altitude_ft_pilot\"\n",
    "    ],\n",
    "    \"Polar H10\": [\n",
    "        \"HR\",\n",
    "        \"RRI\"\n",
    "    ]\n",
    "}\n",
    "b3_save_to = r'placeholder3' # Save Directory\n",
    "\n",
    "# Define column name mappings\n",
    "column_rename_map = {\n",
    "    # VarjoEyeMetrics\n",
    "    \"VarjoEyeMetrics_focusDistance\": \"Focus Distance\",\n",
    "    \"VarjoEyeMetrics_stability\": \"Gaze Stability\",\n",
    "    \"VarjoEyeMetrics_interPupillaryDist\": \"Interpupillary Distance\",\n",
    "    \"VarjoEyeMetrics_leftPupilIrisRatio\": \"Left Pupil Iris Ratio\",\n",
    "    \"VarjoEyeMetrics_rightPupilIrisRatio\": \"Right Pupil Iris Ratio\",\n",
    "    \"VarjoEyeMetrics_leftPupilDiam\": \"Left Pupil Diameter\",\n",
    "    \"VarjoEyeMetrics_rightPupilDiam\": \"Right Pupil Diameter\",\n",
    "    \"VarjoEyeMetrics_leftIrisDiam\": \"Left Iris Diameter\",\n",
    "    \"VarjoEyeMetrics_rightIrisDiam\": \"Right Iris Diameter\",\n",
    "    \"VarjoEyeMetrics_leftOpenness\": \"Left Eye Openness\",\n",
    "    \"VarjoEyeMetrics_rightOpenness\": \"Right Eye Openness\",\n",
    "    \n",
    "    # VarjoGaze\n",
    "    \"VarjoGaze_fwdX\": \"Gaze Forward X\",\n",
    "    \"VarjoGaze_fwdY\": \"Gaze Forward Y\", \n",
    "    \"VarjoGaze_fwdZ\": \"Gaze Forward Z\",\n",
    "    \n",
    "    # XPlaneData (simplified names)\n",
    "    \"XPlaneData_sim/flightmodel/position/latitude\": \"Latitude\",\n",
    "    \"XPlaneData_sim/flightmodel/position/longitude\": \"Longitude\",\n",
    "    \"XPlaneData_sim/cockpit2/gauges/indicators/altitude_ft_pilot\": \"Altitude\",\n",
    "    \n",
    "    # Polar H10 - these will be dynamically matched\n",
    "    \"HR\": \"Heart Rate\",\n",
    "    \"RRI\": \"R-R Interval\"\n",
    "}\n",
    "\n",
    "# Function to rename Polar columns dynamically\n",
    "def rename_polar_columns(df):\n",
    "    \"\"\"Rename any columns containing 'Polar' to standardized names\"\"\"\n",
    "    new_columns = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if 'Polar' in col:\n",
    "            # Extract the actual measurement name (after the last underscore)\n",
    "            measurement = col.split('_')[-1] if '_' in col else col\n",
    "            \n",
    "            # Map to standardized name\n",
    "            if measurement in column_rename_map:\n",
    "                new_columns[col] = column_rename_map[measurement]\n",
    "            else:\n",
    "                # Keep original but clean it up\n",
    "                new_columns[col] = f\"Polar {measurement}\"\n",
    "        else:\n",
    "            # Keep non-Polar columns as they are\n",
    "            new_columns[col] = col\n",
    "    \n",
    "    return df.rename(columns=new_columns)\n",
    "\n",
    "# Loop through all .xdf files and create the .csv dataframe by the same name\n",
    "files = os.listdir(directory)\n",
    "for file in files:\n",
    "    if file.endswith('.xdf'):\n",
    "        # Create full path to the file\n",
    "        filepath = os.path.join(directory, file)\n",
    "\n",
    "        try:\n",
    "            streams, header = pyxdf.load_xdf(filepath)  # Use filepath instead of file\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df_complete = process_xdf_streams(\n",
    "                streams=streams,\n",
    "                selection_dict=selection,\n",
    "                target_freq=4.0,\n",
    "                truncate_n=20,\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            # First rename non-Polar columns using the static mapping\n",
    "            df_complete = df_complete.rename(columns=column_rename_map)\n",
    "            \n",
    "            # Then dynamically rename any Polar columns\n",
    "            df_complete = rename_polar_columns(df_complete)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process streams from {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Create savename by replacing .xdf extension with .csv\n",
    "        savename = file.replace('.xdf', '.csv')\n",
    "        # Save to the b3_save_to directory\n",
    "        savepath = os.path.join(b3_save_to, savename)\n",
    "\n",
    "        try:\n",
    "            df_complete.to_csv(savepath, index=False)  # Save to full path\n",
    "            print(f\"Saved {savename} with {len(df_complete)} rows to {b3_save_to}\")\n",
    "            print(f\"Columns: {list(df_complete.columns)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save {savename}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
